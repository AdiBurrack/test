{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"Sentiment Analysis-Train.csv\")\n",
    "df_test = pd.read_csv(r\"Sentiment Analysis-Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning & Preprocessing\n",
    "\n",
    "### Implemented in Preprocessing.py\n",
    "\n",
    "- seperate numbers from words\n",
    "- turn numbers into 1-DIG, 2-DIG, 3-DIG, 4-DIG, MORE-DIG\n",
    "- remove punctuation\n",
    "- remove usernames\n",
    "- remove urls\n",
    "- remove hashtags\n",
    "\n",
    "Got ideas from: \n",
    "- https://arxiv.org/ftp/arxiv/papers/1702/1702.03197.pdf\n",
    "- https://ac.els-cdn.com/S1877050917321750/1-s2.0-S1877050917321750-main.pdf?_tid=34f291fc-dd9c-11e7-938d-00000aab0f6b&acdnat=1512905043_770e08134d89a36b483dbfbacac3280f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Preprocessing import clean_text\n",
    "\n",
    "df_train = clean_text(df_train)\n",
    "df_test = clean_text(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors -- Countvectorizer / Tfidf\n",
    "\n",
    "Turn the clean text into word vectors (I found that tfidf worked better). \n",
    "\n",
    "Other params to play with - MIN_DF and ngram_range (I took bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "WORD_REPRESENTATION = 'tfidf' # flag to determine if we want word counts or tfidf\n",
    "\n",
    "MIN_DF = 0.001 # threshold for cutting unfrequent terms (terms that appear in less than MIN_DF of tweets)\n",
    "\n",
    "def word_vectors(df_train, df_test):\n",
    "    y = np.array(df_train['Class'].tolist())\n",
    "    count_vect = CountVectorizer(ngram_range=(1,2), min_df = MIN_DF)\n",
    "    train_transcripts = df_train['clean_text'].values\n",
    "    X_counts = count_vect.fit_transform(train_transcripts)\n",
    "    test_transcripts = df_test['clean_text'].values\n",
    "    X_test_counts = count_vect.transform(test_transcripts)\n",
    "    if WORD_REPRESENTATION == 'counts':\n",
    "        return count_vect, X_counts, X_test_counts, y\n",
    "        \n",
    "    else: # we want tfidf\n",
    "        tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n",
    "        X_tfidf = tf_transformer.transform(X_counts)\n",
    "        X_test_tfidf = tf_transformer.transform(X_test_counts)\n",
    "        return count_vect, X_tfidf, X_test_tfidf, y\n",
    "    \n",
    "    \n",
    "count_vect, X, X_test, y = word_vectors(df_train, df_test)\n",
    "\n",
    "NUM_WORDS = X.get_shape()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just examining the data and term frequencies... no use for this block in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "316\n",
      "(u'\\u0627\\u0644\\u0644\\u0647', 116.87475830753655)\n"
     ]
    }
   ],
   "source": [
    "freqs = [(word, X.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sorted_ = sorted (freqs, key = lambda x: -x[1])\n",
    "print(len(sorted_))\n",
    "print(len([x for x in sorted_ if int(x[1])>1]))\n",
    "print(sorted_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=1566\n",
      "xtrain=1566\n",
      "xtest=386\n"
     ]
    }
   ],
   "source": [
    "print \"y=\" + str(len(y))\n",
    "print \"xtrain=\" + str(X.shape[0])\n",
    "print \"xtest=\" + str(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Implemented in featureEngineering.py\n",
    "\n",
    "- contains punctuations\n",
    "- length of tweet\n",
    "- recurring letters\n",
    "- recurring puncts\n",
    "- contains DIG\n",
    "- containg emoji\n",
    "- contains hashtag\n",
    "- containg user\n",
    "- contains url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from featureEngineering import add_features\n",
    "\n",
    "train_features = add_features(df_train)\n",
    "test_features = add_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/datas/lib/python2.7/site-packages/ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Applications/anaconda/envs/datas/lib/python2.7/site-packages/ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n",
      "/Applications/anaconda/envs/datas/lib/python2.7/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_features['tweet_length'].reshape(-1, 1))\n",
    "train_features['tweet_length'] = scaler.transform(train_features['tweet_length'].reshape(-1, 1))\n",
    "test_features['tweet_length'] = scaler.transform(test_features['tweet_length'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge word representation with the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>contains_recurring_letters</th>\n",
       "      <th>contains_DIG</th>\n",
       "      <th>contains_hashtag</th>\n",
       "      <th>contains_user</th>\n",
       "      <th>contains_url</th>\n",
       "      <th>contains_emoji</th>\n",
       "      <th>...</th>\n",
       "      <th>w_يوجد</th>\n",
       "      <th>w_يوجد كفائات</th>\n",
       "      <th>w_يوسف</th>\n",
       "      <th>w_يوفر</th>\n",
       "      <th>w_يوفر لك</th>\n",
       "      <th>w_يوفق</th>\n",
       "      <th>w_يوكل</th>\n",
       "      <th>w_يوم</th>\n",
       "      <th>w_يوم من</th>\n",
       "      <th>w_ﻣﻦ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386 rows × 2520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ?    !    .  tweet_length  contains_recurring_letters  contains_DIG  \\\n",
       "0    0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "1    0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "2    0.0  0.0  0.0      0.027778                         0.0           0.0   \n",
       "3    0.0  0.0  0.0      0.048611                         0.0           0.0   \n",
       "4    0.0  0.0  0.0      0.083333                         0.0           0.0   \n",
       "5    0.0  0.0  0.0      0.062500                         0.0           0.0   \n",
       "6    0.0  0.0  0.0      0.055556                         0.0           0.0   \n",
       "7    0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "8    0.0  0.0  0.0      0.027778                         0.0           0.0   \n",
       "9    0.0  0.0  0.0      0.069444                         0.0           0.0   \n",
       "10   0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "11   0.0  0.0  0.0      0.041667                         0.0           0.0   \n",
       "12   0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "13   0.0  0.0  0.0      0.104167                         0.0           0.0   \n",
       "14   0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "15   0.0  0.0  0.0      0.097222                         0.0           0.0   \n",
       "16   0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "17   0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "18   0.0  0.0  0.0      0.055556                         0.0           0.0   \n",
       "19   0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "20   0.0  0.0  0.0      0.055556                         0.0           0.0   \n",
       "21   0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "22   0.0  0.0  0.0      0.048611                         0.0           0.0   \n",
       "23   0.0  0.0  0.0      0.048611                         0.0           0.0   \n",
       "24   0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "25   0.0  0.0  0.0      0.027778                         0.0           0.0   \n",
       "26   0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "27   0.0  0.0  0.0      0.000000                         0.0           0.0   \n",
       "28   0.0  0.0  0.0      0.083333                         0.0           0.0   \n",
       "29   0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "..   ...  ...  ...           ...                         ...           ...   \n",
       "356  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "357  0.0  0.0  8.0      0.611111                         1.0           0.0   \n",
       "358  0.0  0.0  0.0      0.055556                         0.0           0.0   \n",
       "359  0.0  0.0  0.0      0.048611                         0.0           0.0   \n",
       "360  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "361  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "362  0.0  0.0  0.0      0.041667                         0.0           0.0   \n",
       "363  0.0  0.0  0.0      0.104167                         0.0           0.0   \n",
       "364  0.0  0.0  0.0      0.111111                         0.0           0.0   \n",
       "365  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "366  0.0  0.0  0.0      0.083333                         0.0           0.0   \n",
       "367  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "368  0.0  0.0  0.0      0.048611                         0.0           0.0   \n",
       "369  0.0  0.0  0.0      0.041667                         0.0           0.0   \n",
       "370  0.0  0.0  0.0      0.041667                         0.0           0.0   \n",
       "371  0.0  0.0  0.0      0.027778                         0.0           0.0   \n",
       "372  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "373  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "374  0.0  0.0  1.0      0.138889                         0.0           0.0   \n",
       "375  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "376  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "377  0.0  0.0  0.0      0.041667                         0.0           0.0   \n",
       "378  0.0  0.0  0.0      0.076389                         0.0           0.0   \n",
       "379  0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "380  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "381  0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "382  0.0  0.0  0.0      0.006944                         0.0           0.0   \n",
       "383  0.0  0.0  0.0      0.013889                         0.0           0.0   \n",
       "384  0.0  0.0  0.0      0.034722                         0.0           0.0   \n",
       "385  0.0  0.0  0.0      0.020833                         0.0           0.0   \n",
       "\n",
       "     contains_hashtag  contains_user  contains_url  contains_emoji  ...   \\\n",
       "0                 0.0            0.0           0.0             0.0  ...    \n",
       "1                 0.0            0.0           0.0             0.0  ...    \n",
       "2                 0.0            0.0           0.0             0.0  ...    \n",
       "3                 0.0            0.0           0.0             0.0  ...    \n",
       "4                 0.0            0.0           0.0             0.0  ...    \n",
       "5                 0.0            0.0           0.0             0.0  ...    \n",
       "6                 0.0            0.0           0.0             0.0  ...    \n",
       "7                 0.0            0.0           0.0             0.0  ...    \n",
       "8                 0.0            0.0           0.0             0.0  ...    \n",
       "9                 0.0            0.0           0.0             0.0  ...    \n",
       "10                0.0            0.0           0.0             0.0  ...    \n",
       "11                0.0            0.0           0.0             0.0  ...    \n",
       "12                0.0            0.0           0.0             0.0  ...    \n",
       "13                0.0            0.0           0.0             0.0  ...    \n",
       "14                0.0            0.0           0.0             0.0  ...    \n",
       "15                0.0            0.0           0.0             0.0  ...    \n",
       "16                0.0            0.0           0.0             0.0  ...    \n",
       "17                0.0            0.0           0.0             0.0  ...    \n",
       "18                0.0            0.0           0.0             0.0  ...    \n",
       "19                0.0            0.0           0.0             0.0  ...    \n",
       "20                0.0            0.0           0.0             0.0  ...    \n",
       "21                0.0            0.0           0.0             0.0  ...    \n",
       "22                0.0            0.0           0.0             0.0  ...    \n",
       "23                0.0            0.0           0.0             0.0  ...    \n",
       "24                0.0            0.0           0.0             0.0  ...    \n",
       "25                0.0            0.0           0.0             0.0  ...    \n",
       "26                0.0            0.0           0.0             0.0  ...    \n",
       "27                0.0            0.0           0.0             0.0  ...    \n",
       "28                0.0            0.0           0.0             0.0  ...    \n",
       "29                0.0            0.0           0.0             0.0  ...    \n",
       "..                ...            ...           ...             ...  ...    \n",
       "356               0.0            0.0           0.0             0.0  ...    \n",
       "357               0.0            0.0           0.0             0.0  ...    \n",
       "358               0.0            0.0           0.0             0.0  ...    \n",
       "359               0.0            0.0           0.0             0.0  ...    \n",
       "360               0.0            0.0           0.0             0.0  ...    \n",
       "361               0.0            0.0           0.0             0.0  ...    \n",
       "362               0.0            0.0           0.0             0.0  ...    \n",
       "363               0.0            0.0           0.0             0.0  ...    \n",
       "364               0.0            0.0           0.0             0.0  ...    \n",
       "365               0.0            0.0           0.0             0.0  ...    \n",
       "366               0.0            0.0           0.0             0.0  ...    \n",
       "367               0.0            0.0           0.0             0.0  ...    \n",
       "368               0.0            0.0           0.0             0.0  ...    \n",
       "369               0.0            0.0           0.0             0.0  ...    \n",
       "370               0.0            0.0           0.0             0.0  ...    \n",
       "371               0.0            0.0           0.0             0.0  ...    \n",
       "372               0.0            0.0           0.0             0.0  ...    \n",
       "373               0.0            0.0           0.0             0.0  ...    \n",
       "374               0.0            0.0           0.0             0.0  ...    \n",
       "375               0.0            0.0           0.0             0.0  ...    \n",
       "376               0.0            0.0           0.0             0.0  ...    \n",
       "377               0.0            0.0           0.0             0.0  ...    \n",
       "378               0.0            0.0           0.0             0.0  ...    \n",
       "379               0.0            0.0           0.0             0.0  ...    \n",
       "380               0.0            0.0           0.0             0.0  ...    \n",
       "381               0.0            0.0           0.0             0.0  ...    \n",
       "382               0.0            0.0           0.0             0.0  ...    \n",
       "383               0.0            0.0           0.0             0.0  ...    \n",
       "384               0.0            0.0           0.0             0.0  ...    \n",
       "385               0.0            0.0           0.0             0.0  ...    \n",
       "\n",
       "     w_يوجد  w_يوجد كفائات  w_يوسف  w_يوفر  w_يوفر لك  w_يوفق  w_يوكل  \\\n",
       "0       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "1       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "2       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "3       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "4       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "5       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "6       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "7       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "8       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "9       0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "10      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "11      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "12      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "13      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "14      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "15      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "16      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "17      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "18      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "19      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "20      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "21      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "22      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "23      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "24      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "25      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "26      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "27      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "28      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "29      0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "..      ...            ...     ...     ...        ...     ...     ...   \n",
       "356     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "357     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "358     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "359     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "360     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "361     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "362     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "363     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "364     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "365     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "366     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "367     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "368     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "369     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "370     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "371     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "372     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "373     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "374     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "375     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "376     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "377     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "378     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "379     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "380     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "381     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "382     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "383     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "384     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "385     0.0            0.0     0.0     0.0        0.0     0.0     0.0   \n",
       "\n",
       "        w_يوم  w_يوم من  w_ﻣﻦ  \n",
       "0    0.000000       0.0   0.0  \n",
       "1    0.000000       0.0   0.0  \n",
       "2    0.000000       0.0   0.0  \n",
       "3    0.000000       0.0   0.0  \n",
       "4    0.000000       0.0   0.0  \n",
       "5    0.000000       0.0   0.0  \n",
       "6    0.000000       0.0   0.0  \n",
       "7    0.000000       0.0   0.0  \n",
       "8    0.000000       0.0   0.0  \n",
       "9    0.000000       0.0   0.0  \n",
       "10   0.000000       0.0   0.0  \n",
       "11   0.000000       0.0   0.0  \n",
       "12   0.000000       0.0   0.0  \n",
       "13   0.000000       0.0   0.0  \n",
       "14   0.000000       0.0   0.0  \n",
       "15   0.000000       0.0   0.0  \n",
       "16   0.000000       0.0   0.0  \n",
       "17   0.000000       0.0   0.0  \n",
       "18   0.000000       0.0   0.0  \n",
       "19   0.000000       0.0   0.0  \n",
       "20   0.000000       0.0   0.0  \n",
       "21   0.000000       0.0   0.0  \n",
       "22   0.000000       0.0   0.0  \n",
       "23   0.000000       0.0   0.0  \n",
       "24   0.000000       0.0   0.0  \n",
       "25   0.000000       0.0   0.0  \n",
       "26   0.000000       0.0   0.0  \n",
       "27   0.000000       0.0   0.0  \n",
       "28   0.000000       0.0   0.0  \n",
       "29   0.000000       0.0   0.0  \n",
       "..        ...       ...   ...  \n",
       "356  0.000000       0.0   0.0  \n",
       "357  0.000000       0.0   0.0  \n",
       "358  0.000000       0.0   0.0  \n",
       "359  0.000000       0.0   0.0  \n",
       "360  0.000000       0.0   0.0  \n",
       "361  0.000000       0.0   0.0  \n",
       "362  0.000000       0.0   0.0  \n",
       "363  0.000000       0.0   0.0  \n",
       "364  0.301511       0.0   0.0  \n",
       "365  0.000000       0.0   0.0  \n",
       "366  0.000000       0.0   0.0  \n",
       "367  0.000000       0.0   0.0  \n",
       "368  0.000000       0.0   0.0  \n",
       "369  0.000000       0.0   0.0  \n",
       "370  0.000000       0.0   0.0  \n",
       "371  0.000000       0.0   0.0  \n",
       "372  0.000000       0.0   0.0  \n",
       "373  0.000000       0.0   0.0  \n",
       "374  0.000000       0.0   0.0  \n",
       "375  0.000000       0.0   0.0  \n",
       "376  0.000000       0.0   0.0  \n",
       "377  0.000000       0.0   0.0  \n",
       "378  0.000000       0.0   0.0  \n",
       "379  0.000000       0.0   0.0  \n",
       "380  0.000000       0.0   0.0  \n",
       "381  0.000000       0.0   0.0  \n",
       "382  0.000000       0.0   0.0  \n",
       "383  0.000000       0.0   0.0  \n",
       "384  0.000000       0.0   0.0  \n",
       "385  0.000000       0.0   0.0  \n",
       "\n",
       "[386 rows x 2520 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = ['w_'+x for x in count_vect.get_feature_names()]\n",
    "\n",
    "train_word_counts = pd.DataFrame(X.todense(), columns=feature_columns)\n",
    "train_org_df = pd.merge(train_features, train_word_counts, right_index=True, left_index=True)\n",
    "redundant_columns = ['Text', 'Class', 'Index', 'clean_text']\n",
    "df = train_org_df.drop(redundant_columns, axis=1)\n",
    "\n",
    "\n",
    "test_word_counts = pd.DataFrame(X_test.todense(), columns=feature_columns)\n",
    "test_org_df = pd.merge(test_features, test_word_counts, right_index=True, left_index=True)\n",
    "test_redundant_columns = ['Text', 'Index', 'clean_text']\n",
    "test_df = test_org_df.drop(test_redundant_columns, axis=1)\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use RandomForest for feature selection --> Didn't use this in the end\n",
    "\n",
    "Train the best possible RandomForest on train and look at the most important features\n",
    "Clean the dataset from noisy features and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I STARTED GRID SEARCH...\n",
      "I FINISHED GRID SEARCH!\n",
      "Best parameters set found on development set:\n",
      "{'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 100, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "from RandomForest import RandomForest\n",
    "\n",
    "RF = RandomForest()\n",
    "RF_best_params = RF.optimize_hyperparameters(df,y)\n",
    "RF.fit(df, y, RF_best_params)\n",
    "train_preds = RF.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We will only keep features with relative importance over this threshold\n",
    "THRESH = 0.0001\n",
    "\n",
    "features = df.columns.values\n",
    "NUM_TOP_FEATURES = 100\n",
    "importances = RF._model.feature_importances_\n",
    "indices = np.argsort(abs(importances))[-NUM_TOP_FEATURES:]\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), [importances[x] for x in indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices]) ## removed [indices]\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "indices = np.argsort(np.abs(importances))\n",
    "feat_sorted = [(features[x], importances[x]) for x in indices[::-1]]\n",
    "print(len(feat_sorted))\n",
    "new_columns = [x for x in feat_sorted if x[1]>=0.0001]\n",
    "columns_to_drop = [x[0] for x in feat_sorted if x[1]<0.0001]\n",
    "print(len(new_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: أنا متأكده أنو هذولي مدفوعلهم علشان يجون يغنون لأنو مافي ناس مهابيل لهالدرجه\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_significant_features(model, X, transcript, NUM_TOP_FEATURES = 10, value_mod_func=None):\n",
    "    features = X.index.values\n",
    "    importances = model._model.feature_importances_\n",
    "    importances = importances * X.values\n",
    "    \n",
    "    indices = np.argsort(np.abs(importances))[-NUM_TOP_FEATURES:]\n",
    "\n",
    "    print(\"{}: {}\".format(int(model.predict([X])), transcript))\n",
    "\n",
    "    h = importances[indices]\n",
    "    if value_mod_func is not None:\n",
    "        array_value_mod_func = np.vectorize(value_mod_func)\n",
    "        h = array_value_mod_func(h)\n",
    "    plt.barh(range(len(indices)), h, color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), features[indices]) ## removed [indices]\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return [{\n",
    "        'index': x,\n",
    "        'value': X.values[x],\n",
    "        'feature': features[x],\n",
    "        'importance': importances[x],\n",
    "        # 'modified_importance': value_mod_func([importances[x]])\n",
    "    } for x in indices[::-1]]\n",
    "\n",
    "i=4; plot_significant_features(RF, df.iloc[i], test_org_df.iloc[i]['Text'], 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaner_df = df.drop(columns_to_drop, axis=1)\n",
    "cleaner_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation to choose best model\n",
    "\n",
    "Since there is little data, I prefer to choose a model on cross validation split, then optimize hyperparameters and train.\n",
    "\n",
    "I experimented with:\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Kernel SVM\n",
    "- RandomForest\n",
    "\n",
    "Each classifier implemented in its own .py file , inheriting from classifier.py (mostly for my convinience...).\n",
    "\n",
    "I found that SVM performs best, with scores of:\n",
    "\n",
    "Train mean: 0.968977 and std: 0.001392\n",
    "\n",
    "Dev mean: 0.845603 and std: 0.045575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinSVM\n",
      "Train mean: 0.968561 and std: 0.001341\n",
      "Dev mean: 0.847574 and std: 0.045600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from LinearSVM import LinearSVM\n",
    "from LogReg import LogReg\n",
    "from NaiveBayes import NaiveBayes\n",
    "from RandomForest import RandomForest\n",
    "\n",
    "# Linear SVM\n",
    "print(\"LinSVM\")\n",
    "LinSVM = LinearSVM()\n",
    "LinSVM_scores = LinSVM.cross_validation(cleaner_df, y)\n",
    "#print(\"LinSVM mean: %f and std: %f\"%(LinSVM_scores[0],LinSVM_scores[1]))\n",
    "\n",
    "\n",
    "# logistic Regression\n",
    "# print(\"LogReg\")\n",
    "# LogReg = LogReg()\n",
    "# LogReg_scores = LogReg.cross_validation(cleaner_df, y)\n",
    "#print(\"LogReg mean: %f and std: %f\"%(LogReg_scores[0],LogReg_scores[1]))\n",
    "\n",
    "\n",
    "# # Naive Bayes\n",
    "# NB = NaiveBayes()\n",
    "# NB_scores = NB.cross_validation(df, y)\n",
    "# print(\"Naive Bayes mean: %f and std: %f\"%(NB_scores[0],NB_scores[1]))\n",
    "\n",
    "\n",
    "# # RandomForest\n",
    "# RandFor = RandomForest()\n",
    "# RF_scores = RandFor.cross_validation(cleaner_df, y)\n",
    "# #print(\"Random Forest mean: %f and std: %f\"%(RF_scores[0],RF_scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking feature importance on LinSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    " coef = classifier.coef_.ravel()\n",
    " top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    " top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    " top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    " # create plot\n",
    " plt.figure(figsize=(15, 5))\n",
    " colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    " plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    " feature_names = np.array(feature_names)\n",
    " plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right')\n",
    " plt.show()\n",
    " return feature_names[top_positive_coefficients], feature_names[top_negative_coefficients]\n",
    "\n",
    "top_positive , top_negative = plot_coefficients(LinSVM._model, df.columns)\n",
    "\n",
    "positive = [x.replace('w_','') for x in top_positive]\n",
    "negative = [x.replace('w_','') for x in top_negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Google translate I found that among terms that most point to a positive sentiment are:\n",
    "- 'Fulfillment'\n",
    "- 'I love'\n",
    "- 'Good'\n",
    "- 'resolution'\n",
    "\n",
    "along with a lot of references to god:\n",
    "- 'my Lord'\n",
    "- 'Allah'\n",
    "- 'O Lord'\n",
    "- 'O Allah'\n",
    "- 'Our Lord'\n",
    "\n",
    "### Among the terms that most point to a negative sentiment are:\n",
    "- 'unjust'\n",
    "- 'Stupid'\n",
    "- 'Why'\n",
    "- 'Haram'\n",
    "- 'Problems'\n",
    "- 'Nuclear'\n",
    "- 'Lost'\n",
    "- 'Junk'\n",
    "- 'Damned'\n",
    "- ... also, it seems that longer tweets tend to be negative.\n",
    "\n",
    "### While it's still google translate so it not foolproof, we do see that some context is captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we've chosen a Linear Model... (LinSVM)\n",
    "\n",
    "## Let's perform Hyper Parameter Optimization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I STARTED GRID SEARCH...\n",
      "I FINISHED GRID SEARCH!\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'C': 1}\n",
      "I've returned your trained model :)\n"
     ]
    }
   ],
   "source": [
    "from LinearSVM import LinearSVM\n",
    "LinSVM = LinearSVM()\n",
    "LinSVM_best_params = LinSVM.optimize_hyperparameters(df,y)\n",
    "LinSVM.fit(df, y, LinSVM_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = LinSVM.predict(test_df)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ ~ ~ \n",
    "# Now let's see how an LSTM performs\n",
    "\n",
    "I implement the Keras classifier slightly different (i.e, explicitly) unlike the other classifiers, since they are all Sklearn where Keras requires a wrapper that I don't like that much. Note that this model is based on binary occurences of unigrams (unlike the linear models, with bigram tfidf).\n",
    "\n",
    "For this reason (+ runtime efficiency) I use the keras tokenizer instead of the Vectorizers from before, and also implement CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#NUM_WORDS = 2000 #hyper parameter to play with, take NUM_WORDS top-frequent word\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(df_train['clean_text'].values)\n",
    "X = tokenizer.texts_to_sequences(df_train['clean_text'].values)\n",
    "X = pad_sequences(X)\n",
    "y = np.array(df_train['Class'].tolist())\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(df_test['clean_text'].values)\n",
    "X_test = pad_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Training on fold 1/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.6867 - acc: 0.5481 - val_loss: 0.6604 - val_acc: 0.7342\n",
      "******\n",
      "f1: 0.718703880686\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.5487 - acc: 0.7882 - val_loss: 0.4618 - val_acc: 0.8354\n",
      "******\n",
      "f1: 0.834010954824\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.3026 - acc: 0.8931 - val_loss: 0.3417 - val_acc: 0.8354\n",
      "******\n",
      "f1: 0.834702862081\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 20s 13ms/step - loss: 0.1845 - acc: 0.9395 - val_loss: 0.2988 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.886112481049\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 19s 13ms/step - loss: 0.1335 - acc: 0.9570 - val_loss: 0.3108 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.1046 - acc: 0.9623 - val_loss: 0.3300 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.0857 - acc: 0.9664 - val_loss: 0.3533 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.0788 - acc: 0.9670 - val_loss: 0.4043 - val_acc: 0.8354\n",
      "******\n",
      "f1: 0.834010954824\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0699 - acc: 0.9711 - val_loss: 0.4078 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0599 - acc: 0.9731 - val_loss: 0.4539 - val_acc: 0.8101\n",
      "******\n",
      "f1: 0.807421570014\n",
      "******\n",
      "Build model...\n",
      "Training on fold 2/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.6848 - acc: 0.5245 - val_loss: 0.6542 - val_acc: 0.5696\n",
      "******\n",
      "f1: 0.458491820997\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.5372 - acc: 0.7613 - val_loss: 0.4369 - val_acc: 0.8101\n",
      "******\n",
      "f1: 0.810004575263\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.2944 - acc: 0.8998 - val_loss: 0.3417 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.886112481049\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.1879 - acc: 0.9354 - val_loss: 0.3411 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.859172319686\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.1417 - acc: 0.9482 - val_loss: 0.3154 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.898734177215\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.1094 - acc: 0.9583 - val_loss: 0.2991 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.886112481049\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0933 - acc: 0.9576 - val_loss: 0.3125 - val_acc: 0.9114\n",
      "******\n",
      "f1: 0.911135364498\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0878 - acc: 0.9630 - val_loss: 0.2947 - val_acc: 0.9241\n",
      "******\n",
      "f1: 0.923707192621\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0786 - acc: 0.9657 - val_loss: 0.3274 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.88574546864\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0673 - acc: 0.9711 - val_loss: 0.3136 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89784645734\n",
      "******\n",
      "Build model...\n",
      "Training on fold 3/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.6821 - acc: 0.5602 - val_loss: 0.6560 - val_acc: 0.8101\n",
      "******\n",
      "f1: 0.810004575263\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.5331 - acc: 0.8137 - val_loss: 0.4240 - val_acc: 0.8481\n",
      "******\n",
      "f1: 0.848149951315\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.2990 - acc: 0.8985 - val_loss: 0.3249 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860355572782\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 15s 10ms/step - loss: 0.1962 - acc: 0.9294 - val_loss: 0.2820 - val_acc: 0.8734\n",
      "******\n",
      "f1: 0.872308071675\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 15s 10ms/step - loss: 0.1451 - acc: 0.9462 - val_loss: 0.2273 - val_acc: 0.9114\n",
      "******\n",
      "f1: 0.911135364498\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 15s 10ms/step - loss: 0.1119 - acc: 0.9576 - val_loss: 0.2296 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.898276256828\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0939 - acc: 0.9583 - val_loss: 0.2177 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89857126566\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.0774 - acc: 0.9704 - val_loss: 0.2476 - val_acc: 0.8734\n",
      "******\n",
      "f1: 0.872845321035\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.0754 - acc: 0.9697 - val_loss: 0.2429 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.898276256828\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 20s 13ms/step - loss: 0.0687 - acc: 0.9691 - val_loss: 0.2788 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.885338218749\n",
      "******\n",
      "Build model...\n",
      "Training on fold 4/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 20s 13ms/step - loss: 0.6836 - acc: 0.5508 - val_loss: 0.6677 - val_acc: 0.5316\n",
      "******\n",
      "f1: 0.401791203891\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.5394 - acc: 0.8003 - val_loss: 0.4887 - val_acc: 0.7848\n",
      "******\n",
      "f1: 0.78384220426\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 20s 14ms/step - loss: 0.3049 - acc: 0.9005 - val_loss: 0.4477 - val_acc: 0.7975\n",
      "******\n",
      "f1: 0.796165061988\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.1895 - acc: 0.9294 - val_loss: 0.4603 - val_acc: 0.7975\n",
      "******\n",
      "f1: 0.797533268419\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.1420 - acc: 0.9442 - val_loss: 0.4216 - val_acc: 0.8481\n",
      "******\n",
      "f1: 0.848003894839\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.1134 - acc: 0.9583 - val_loss: 0.4128 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 19s 13ms/step - loss: 0.0870 - acc: 0.9657 - val_loss: 0.4252 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860536244502\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.0719 - acc: 0.9711 - val_loss: 0.4948 - val_acc: 0.8354\n",
      "******\n",
      "f1: 0.834965676924\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0699 - acc: 0.9724 - val_loss: 0.5227 - val_acc: 0.8228\n",
      "******\n",
      "f1: 0.822671210646\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.0678 - acc: 0.9697 - val_loss: 0.5450 - val_acc: 0.8228\n",
      "******\n",
      "f1: 0.822671210646\n",
      "******\n",
      "Build model...\n",
      "Training on fold 5/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.6833 - acc: 0.5427 - val_loss: 0.6673 - val_acc: 0.7848\n",
      "******\n",
      "f1: 0.784185885209\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.5449 - acc: 0.8393 - val_loss: 0.4902 - val_acc: 0.8228\n",
      "******\n",
      "f1: 0.822841609867\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.3200 - acc: 0.8971 - val_loss: 0.3982 - val_acc: 0.8101\n",
      "******\n",
      "f1: 0.809575781067\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.1939 - acc: 0.9354 - val_loss: 0.3554 - val_acc: 0.8101\n",
      "******\n",
      "f1: 0.810187468415\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.1377 - acc: 0.9496 - val_loss: 0.3355 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860759493671\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 17s 11ms/step - loss: 0.1056 - acc: 0.9630 - val_loss: 0.2973 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.86067002186\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0909 - acc: 0.9670 - val_loss: 0.3326 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.86067002186\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.0780 - acc: 0.9731 - val_loss: 0.3226 - val_acc: 0.8734\n",
      "******\n",
      "f1: 0.873214082075\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.0704 - acc: 0.9657 - val_loss: 0.3767 - val_acc: 0.8608\n",
      "******\n",
      "f1: 0.860759493671\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.0624 - acc: 0.9731 - val_loss: 0.3738 - val_acc: 0.8861\n",
      "******\n",
      "f1: 0.886112481049\n",
      "******\n",
      "Build model...\n",
      "Training on fold 6/20...\n",
      "Train on 1487 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.6810 - acc: 0.5743 - val_loss: 0.6306 - val_acc: 0.6962\n",
      "******\n",
      "f1: 0.660487589793\n",
      "******\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.5115 - acc: 0.8016 - val_loss: 0.3518 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89876663421\n",
      "******\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.3009 - acc: 0.8857 - val_loss: 0.2293 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89784645734\n",
      "******\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.1971 - acc: 0.9314 - val_loss: 0.1873 - val_acc: 0.8734\n",
      "******\n",
      "f1: 0.871597584181\n",
      "******\n",
      "Epoch 5/10\n",
      "1487/1487 [==============================] - 18s 12ms/step - loss: 0.1451 - acc: 0.9489 - val_loss: 0.1693 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89784645734\n",
      "******\n",
      "Epoch 6/10\n",
      "1487/1487 [==============================] - 20s 13ms/step - loss: 0.1150 - acc: 0.9556 - val_loss: 0.1736 - val_acc: 0.9114\n",
      "******\n",
      "f1: 0.910818614583\n",
      "******\n",
      "Epoch 7/10\n",
      "1487/1487 [==============================] - 16s 11ms/step - loss: 0.0975 - acc: 0.9623 - val_loss: 0.1843 - val_acc: 0.8987\n",
      "******\n",
      "f1: 0.89784645734\n",
      "******\n",
      "Epoch 8/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0798 - acc: 0.9691 - val_loss: 0.1680 - val_acc: 0.9241\n",
      "******\n",
      "f1: 0.923707192621\n",
      "******\n",
      "Epoch 9/10\n",
      "1487/1487 [==============================] - 19s 13ms/step - loss: 0.0717 - acc: 0.9711 - val_loss: 0.1841 - val_acc: 0.9114\n",
      "******\n",
      "f1: 0.910818614583\n",
      "******\n",
      "Epoch 10/10\n",
      "1487/1487 [==============================] - 17s 12ms/step - loss: 0.0675 - acc: 0.9697 - val_loss: 0.1690 - val_acc: 0.9241\n",
      "******\n",
      "f1: 0.923707192621\n",
      "******\n",
      "Build model...\n",
      "Training on fold 7/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.6819 - acc: 0.5423 - val_loss: 0.6547 - val_acc: 0.5513\n",
      "******\n",
      "f1: 0.42803926367\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.5398 - acc: 0.7897 - val_loss: 0.4838 - val_acc: 0.7692\n",
      "******\n",
      "f1: 0.768926525024\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.3088 - acc: 0.8952 - val_loss: 0.3781 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904808272\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.2030 - acc: 0.9308 - val_loss: 0.3581 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884558479495\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1415 - acc: 0.9489 - val_loss: 0.3907 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1098 - acc: 0.9624 - val_loss: 0.3629 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833360732095\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0921 - acc: 0.9624 - val_loss: 0.4417 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833251028807\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0801 - acc: 0.9671 - val_loss: 0.4951 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.819800569801\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0694 - acc: 0.9731 - val_loss: 0.5607 - val_acc: 0.7949\n",
      "******\n",
      "f1: 0.793236221143\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0657 - acc: 0.9718 - val_loss: 0.4844 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.82027618613\n",
      "******\n",
      "Build model...\n",
      "Training on fold 8/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.6854 - acc: 0.5336 - val_loss: 0.6638 - val_acc: 0.5385\n",
      "******\n",
      "f1: 0.42047426258\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.5497 - acc: 0.7688 - val_loss: 0.4097 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.870777370777\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.2973 - acc: 0.8958 - val_loss: 0.3365 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904716682\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1816 - acc: 0.9341 - val_loss: 0.3355 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871625847236\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1286 - acc: 0.9536 - val_loss: 0.3430 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871794871795\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1029 - acc: 0.9610 - val_loss: 0.3796 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0804 - acc: 0.9671 - val_loss: 0.3798 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0748 - acc: 0.9698 - val_loss: 0.3996 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833360732095\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0642 - acc: 0.9698 - val_loss: 0.4539 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.844097079391\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0650 - acc: 0.9718 - val_loss: 0.4279 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833360732095\n",
      "******\n",
      "Build model...\n",
      "Training on fold 9/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 19s 12ms/step - loss: 0.6835 - acc: 0.5712 - val_loss: 0.6676 - val_acc: 0.7308\n",
      "******\n",
      "f1: 0.707359966766\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.5283 - acc: 0.8340 - val_loss: 0.4990 - val_acc: 0.7436\n",
      "******\n",
      "f1: 0.741554741555\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.2928 - acc: 0.8945 - val_loss: 0.4291 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.819802917364\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.1767 - acc: 0.9335 - val_loss: 0.3960 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858626147515\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1283 - acc: 0.9543 - val_loss: 0.4266 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833251137049\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1073 - acc: 0.9577 - val_loss: 0.4413 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0877 - acc: 0.9677 - val_loss: 0.4624 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.82027618613\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0742 - acc: 0.9704 - val_loss: 0.4890 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.820276653171\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0655 - acc: 0.9718 - val_loss: 0.5611 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.819802917364\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0632 - acc: 0.9691 - val_loss: 0.5457 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.82027618613\n",
      "******\n",
      "Build model...\n",
      "Training on fold 10/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 20s 13ms/step - loss: 0.6859 - acc: 0.5235 - val_loss: 0.6629 - val_acc: 0.5641\n",
      "******\n",
      "f1: 0.467865467865\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 20s 14ms/step - loss: 0.5377 - acc: 0.8138 - val_loss: 0.4244 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.845543345543\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.2918 - acc: 0.8918 - val_loss: 0.3760 - val_acc: 0.7949\n",
      "******\n",
      "f1: 0.794871794872\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1896 - acc: 0.9281 - val_loss: 0.3996 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871286121286\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.1331 - acc: 0.9496 - val_loss: 0.4026 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1109 - acc: 0.9577 - val_loss: 0.5115 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807723921648\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0906 - acc: 0.9644 - val_loss: 0.4490 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0772 - acc: 0.9704 - val_loss: 0.5587 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904716682\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0785 - acc: 0.9671 - val_loss: 0.4810 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858625227993\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0686 - acc: 0.9691 - val_loss: 0.5329 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871625847236\n",
      "******\n",
      "Build model...\n",
      "Training on fold 11/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 20s 14ms/step - loss: 0.6825 - acc: 0.5464 - val_loss: 0.6444 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884634352989\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.5543 - acc: 0.8085 - val_loss: 0.4578 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897300944669\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 20s 13ms/step - loss: 0.3411 - acc: 0.8804 - val_loss: 0.3217 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858626147515\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.2115 - acc: 0.9288 - val_loss: 0.2725 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884558479495\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1455 - acc: 0.9476 - val_loss: 0.2675 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871794871795\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1187 - acc: 0.9550 - val_loss: 0.2477 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884634352989\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 19s 12ms/step - loss: 0.0982 - acc: 0.9617 - val_loss: 0.2741 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.0869 - acc: 0.9644 - val_loss: 0.2647 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0762 - acc: 0.9651 - val_loss: 0.2746 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0649 - acc: 0.9745 - val_loss: 0.3276 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Build model...\n",
      "Training on fold 12/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.6841 - acc: 0.5269 - val_loss: 0.6703 - val_acc: 0.5641\n",
      "******\n",
      "f1: 0.504228350266\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.5360 - acc: 0.7903 - val_loss: 0.4774 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.776824606441\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.3195 - acc: 0.8952 - val_loss: 0.4075 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.819802917364\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.1988 - acc: 0.9382 - val_loss: 0.4113 - val_acc: 0.7949\n",
      "******\n",
      "f1: 0.794871794872\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1449 - acc: 0.9530 - val_loss: 0.4613 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.779763964172\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1227 - acc: 0.9543 - val_loss: 0.4964 - val_acc: 0.7949\n",
      "******\n",
      "f1: 0.794871794872\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0910 - acc: 0.9671 - val_loss: 0.5551 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.78078728783\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0849 - acc: 0.9657 - val_loss: 0.5975 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807597340931\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0690 - acc: 0.9751 - val_loss: 0.6444 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.781511715989\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0630 - acc: 0.9745 - val_loss: 0.6701 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807597340931\n",
      "******\n",
      "Build model...\n",
      "Training on fold 13/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 20s 14ms/step - loss: 0.6859 - acc: 0.5612 - val_loss: 0.6663 - val_acc: 0.7308\n",
      "******\n",
      "f1: 0.707359966766\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.5437 - acc: 0.8058 - val_loss: 0.4696 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807597340931\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.2994 - acc: 0.9079 - val_loss: 0.3496 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.830566269775\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1913 - acc: 0.9321 - val_loss: 0.2905 - val_acc: 0.9231\n",
      "******\n",
      "f1: 0.922975508341\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 19s 12ms/step - loss: 0.1395 - acc: 0.9483 - val_loss: 0.2628 - val_acc: 0.9103\n",
      "******\n",
      "f1: 0.910034235996\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.1125 - acc: 0.9603 - val_loss: 0.2771 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897028897029\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0926 - acc: 0.9664 - val_loss: 0.2798 - val_acc: 0.9103\n",
      "******\n",
      "f1: 0.910034235996\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0828 - acc: 0.9664 - val_loss: 0.3382 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.856632997502\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0980 - acc: 0.9583 - val_loss: 0.2762 - val_acc: 0.9103\n",
      "******\n",
      "f1: 0.910271163436\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0690 - acc: 0.9677 - val_loss: 0.2806 - val_acc: 0.9103\n",
      "******\n",
      "f1: 0.910212092434\n",
      "******\n",
      "Build model...\n",
      "Training on fold 14/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.6847 - acc: 0.5208 - val_loss: 0.6628 - val_acc: 0.6795\n",
      "******\n",
      "f1: 0.674165903413\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.5517 - acc: 0.7923 - val_loss: 0.5224 - val_acc: 0.7436\n",
      "******\n",
      "f1: 0.736273690078\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.3081 - acc: 0.9126 - val_loss: 0.4739 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.782087111201\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1982 - acc: 0.9335 - val_loss: 0.5243 - val_acc: 0.7692\n",
      "******\n",
      "f1: 0.768926525024\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.1540 - acc: 0.9456 - val_loss: 0.4987 - val_acc: 0.7692\n",
      "******\n",
      "f1: 0.768927125506\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1154 - acc: 0.9590 - val_loss: 0.5001 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.85815648036\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0916 - acc: 0.9630 - val_loss: 0.5136 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833251028807\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0898 - acc: 0.9624 - val_loss: 0.5409 - val_acc: 0.7949\n",
      "******\n",
      "f1: 0.794060476987\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0831 - acc: 0.9644 - val_loss: 0.5282 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.820512820513\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0652 - acc: 0.9718 - val_loss: 0.5569 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.820512820513\n",
      "******\n",
      "Build model...\n",
      "Training on fold 15/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.6828 - acc: 0.5376 - val_loss: 0.6537 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807216219991\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.5203 - acc: 0.8219 - val_loss: 0.4115 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.805684787458\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.2977 - acc: 0.8999 - val_loss: 0.3357 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 19s 12ms/step - loss: 0.1993 - acc: 0.9328 - val_loss: 0.3316 - val_acc: 0.8974\n",
      "******\n",
      "f1: 0.897435897436\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.1447 - acc: 0.9503 - val_loss: 0.3360 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904808272\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.1100 - acc: 0.9577 - val_loss: 0.3189 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871794871795\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0903 - acc: 0.9657 - val_loss: 0.3257 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871794871795\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0792 - acc: 0.9664 - val_loss: 0.3252 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884558404558\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0730 - acc: 0.9704 - val_loss: 0.3202 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.844927165857\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.0606 - acc: 0.9751 - val_loss: 0.3438 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904716682\n",
      "******\n",
      "Build model...\n",
      "Training on fold 16/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 20s 13ms/step - loss: 0.6848 - acc: 0.5343 - val_loss: 0.6524 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.816886816887\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.5399 - acc: 0.8165 - val_loss: 0.3733 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884558479495\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.2985 - acc: 0.9046 - val_loss: 0.2548 - val_acc: 0.9103\n",
      "******\n",
      "f1: 0.910212150718\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1846 - acc: 0.9395 - val_loss: 0.2283 - val_acc: 0.8846\n",
      "******\n",
      "f1: 0.884634352989\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.1446 - acc: 0.9503 - val_loss: 0.2385 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858904808272\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.1123 - acc: 0.9583 - val_loss: 0.2475 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871794871795\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0866 - acc: 0.9671 - val_loss: 0.2551 - val_acc: 0.8718\n",
      "******\n",
      "f1: 0.871625847236\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.0817 - acc: 0.9671 - val_loss: 0.2822 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.846153846154\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.0721 - acc: 0.9664 - val_loss: 0.3058 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858997542542\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.0641 - acc: 0.9718 - val_loss: 0.3217 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858997542542\n",
      "******\n",
      "Build model...\n",
      "Training on fold 17/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.6860 - acc: 0.5195 - val_loss: 0.6709 - val_acc: 0.5385\n",
      "******\n",
      "f1: 0.402387267905\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.5575 - acc: 0.8011 - val_loss: 0.4529 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807216219991\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.3078 - acc: 0.8958 - val_loss: 0.3540 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.833251028807\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.1897 - acc: 0.9415 - val_loss: 0.3471 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.832920723992\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1451 - acc: 0.9469 - val_loss: 0.3627 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.832366749517\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1175 - acc: 0.9530 - val_loss: 0.3597 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.832920723992\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0957 - acc: 0.9644 - val_loss: 0.3619 - val_acc: 0.8462\n",
      "******\n",
      "f1: 0.845951016683\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.0761 - acc: 0.9691 - val_loss: 0.3958 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.820512820513\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0685 - acc: 0.9724 - val_loss: 0.4549 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.807216219991\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0639 - acc: 0.9711 - val_loss: 0.4702 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.82027618613\n",
      "******\n",
      "Build model...\n",
      "Training on fold 18/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.6849 - acc: 0.5329 - val_loss: 0.6666 - val_acc: 0.6282\n",
      "******\n",
      "f1: 0.581340228399\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.5516 - acc: 0.7897 - val_loss: 0.5199 - val_acc: 0.7692\n",
      "******\n",
      "f1: 0.769230769231\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 17s 11ms/step - loss: 0.3190 - acc: 0.8911 - val_loss: 0.4300 - val_acc: 0.7692\n",
      "******\n",
      "f1: 0.767399267399\n",
      "******\n",
      "Epoch 4/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1921 - acc: 0.9341 - val_loss: 0.4238 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.781511715989\n",
      "******\n",
      "Epoch 5/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1415 - acc: 0.9503 - val_loss: 0.4283 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.819800569801\n",
      "******\n",
      "Epoch 6/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.1116 - acc: 0.9617 - val_loss: 0.4587 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.806577018673\n",
      "******\n",
      "Epoch 7/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.0931 - acc: 0.9637 - val_loss: 0.4738 - val_acc: 0.7821\n",
      "******\n",
      "f1: 0.78078728783\n",
      "******\n",
      "Epoch 8/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0797 - acc: 0.9677 - val_loss: 0.5091 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.832920723992\n",
      "******\n",
      "Epoch 9/10\n",
      "1488/1488 [==============================] - 19s 13ms/step - loss: 0.0699 - acc: 0.9684 - val_loss: 0.5448 - val_acc: 0.8333\n",
      "******\n",
      "f1: 0.832920723992\n",
      "******\n",
      "Epoch 10/10\n",
      "1488/1488 [==============================] - 18s 12ms/step - loss: 0.0626 - acc: 0.9738 - val_loss: 0.5527 - val_acc: 0.8077\n",
      "******\n",
      "f1: 0.806577018673\n",
      "******\n",
      "Build model...\n",
      "Training on fold 19/20...\n",
      "Train on 1488 samples, validate on 78 samples\n",
      "Epoch 1/10\n",
      "1488/1488 [==============================] - 21s 14ms/step - loss: 0.6812 - acc: 0.5591 - val_loss: 0.6523 - val_acc: 0.6282\n",
      "******\n",
      "f1: 0.581340228399\n",
      "******\n",
      "Epoch 2/10\n",
      "1488/1488 [==============================] - 17s 12ms/step - loss: 0.5094 - acc: 0.7910 - val_loss: 0.4385 - val_acc: 0.8205\n",
      "******\n",
      "f1: 0.820512820513\n",
      "******\n",
      "Epoch 3/10\n",
      "1488/1488 [==============================] - 16s 11ms/step - loss: 0.2793 - acc: 0.9012 - val_loss: 0.3496 - val_acc: 0.8590\n",
      "******\n",
      "f1: 0.858625227993\n",
      "******\n",
      "Epoch 4/10\n",
      "1152/1488 [======================>.......] - ETA: 3s - loss: 0.1744 - acc: 0.9462"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_predict = val_predict.transpose()[0]\n",
    "        val_targ = np.array(self.validation_data[1].ravel())\n",
    "        res = precision_recall_fscore_support(val_targ, val_predict, average='weighted')\n",
    "        print(\"******\")\n",
    "        print(\"f1: \" + str(res[2]))\n",
    "        print(\"******\")\n",
    "        return \n",
    "\n",
    "    \n",
    "params = {'batch_size': 32,\n",
    "         'num_splits': 20,\n",
    "         'embedding_size': 64,\n",
    "          'hidden_size': 64,\n",
    "          'dropout':0.2 ,\n",
    "          'recurrent_dropout': 0.2, \n",
    "          'epochs': 10,\n",
    "          'num_words': NUM_WORDS\n",
    "         }\n",
    "\n",
    "\n",
    "def lstm_cross_validation(X, y, params):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=params['num_splits'], shuffle=True)\n",
    "    train_res = []\n",
    "    dev_res = []\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, params['embedding_size']))\n",
    "        model.add(LSTM(params['hidden_size'], dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout']))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "        # try using different optimizers and different optimizer configs\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        print(\"Training on fold \" + str(index+1) + \"/\"+ str(params['num_splits'])+\"...\")\n",
    "        # Generate batches from indices\n",
    "        X_train, X_dev = X[train_indices], X[val_indices]\n",
    "        y_train, y_dev = y[train_indices], y[val_indices]\n",
    "\n",
    "        model.fit(X_train, y_train, batch_size=params['batch_size'], epochs=params['epochs'], \n",
    "                  callbacks=[Metrics()], validation_data = (X_dev, y_dev))\n",
    "\n",
    "        train_preds = model.predict_classes(X_train)\n",
    "        dev_preds = model.predict_classes(X_dev)\n",
    "        train_res.append(f1_score(y_train, train_preds))\n",
    "        dev_res.append(f1_score(y_dev, dev_preds))\n",
    "        \n",
    "    print(\"Train LSTM mean: %f and std: %f\"%(np.mean(train_res), np.std(train_res)))\n",
    "    print(\"Dev LSTM mean: %f and std: %f\"%(np.mean(dev_res), np.std(dev_res)))\n",
    "    \n",
    "lstm_cross_validation(X, y, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LSTM gets the following results:\n",
    "\n",
    "Train LSTM mean: 0.976330 and std: 0.002420\n",
    "\n",
    "Dev LSTM mean: 0.860143 and std: 0.042228\n",
    "\n",
    "I present the train mean to asses overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1566/1566 [==============================] - 6s - loss: 0.6828 - acc: 0.5409     \n",
      "Epoch 2/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.5164 - acc: 0.7976     \n",
      "Epoch 3/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.2987 - acc: 0.9010     \n",
      "Epoch 4/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.1874 - acc: 0.9374     \n",
      "Epoch 5/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.1389 - acc: 0.9464     \n",
      "Epoch 6/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.1107 - acc: 0.9572     \n",
      "Epoch 7/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.0899 - acc: 0.9642     \n",
      "Epoch 8/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.0760 - acc: 0.9732     \n",
      "Epoch 9/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.0731 - acc: 0.9674     \n",
      "Epoch 10/10\n",
      "1566/1566 [==============================] - 4s - loss: 0.0616 - acc: 0.9732     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27d99ed5e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, params['embedding_size']))\n",
    "model.add(LSTM(params['hidden_size'], dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout']))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=params['batch_size'], epochs=params['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/386 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_test_preds = model.predict_classes(X_test)\n",
    "keras_test_preds = keras_test_preds.ravel()\n",
    "keras_test_preds"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (datas)",
   "language": "python",
   "name": "datas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
